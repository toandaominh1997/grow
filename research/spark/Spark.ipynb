{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae701f6-0893-4803-a2a5-78e4787e8c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/19 16:36:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"Iris\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"grow\").master(\"spark://172.18.0.3:7077\").getOrCreate()\n",
    "sc = pyspark.SparkConf()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e51e84d-bc42-4fd5-b9ae-a7a0998db1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              5.1|             3.5|              1.4|             0.2|     0|\n",
      "|              4.9|             3.0|              1.4|             0.2|     0|\n",
      "|              4.7|             3.2|              1.3|             0.2|     0|\n",
      "|              4.6|             3.1|              1.5|             0.2|     0|\n",
      "|              5.0|             3.6|              1.4|             0.2|     0|\n",
      "|              5.4|             3.9|              1.7|             0.4|     0|\n",
      "|              4.6|             3.4|              1.4|             0.3|     0|\n",
      "|              5.0|             3.4|              1.5|             0.2|     0|\n",
      "|              4.4|             2.9|              1.4|             0.2|     0|\n",
      "|              4.9|             3.1|              1.5|             0.1|     0|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame= True)\n",
    "df = X\n",
    "df['target'] = y\n",
    "df = spark.createDataFrame(df)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1adda-9495-499f-b304-161cf34bf63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize all numerical columns into a single feature column\n",
    "feature_cols = df.columns[:-1]\n",
    "assembler = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = assembler.transform(df)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41dfa73-7488-4134-b5d8-c136cfd2cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text labels into indices\n",
    "data = data.select(['features', 'target'])\n",
    "label_indexer = pyspark.ml.feature.StringIndexer(inputCol='target', outputCol='label').fit(data)\n",
    "data = label_indexer.transform(data)\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0236302-1bd0-4633-869f-d2f3347e4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the features and label column\n",
    "data = data.select(['features', 'label'])\n",
    "print(\"Reading for machine learning\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162df78-415a-4707-ae9e-4536e1a87e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe0fda-45dc-4a62-93e6-68817ed4358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "pipe = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "pipe = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a40c4a-c58b-448e-bef8-ece2be5eedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.transform(test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9109c08-eee3-400b-b4b2-d3f9e0eb185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = pyspark.ml.evaluation.MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4afc62-7eef-4cf7-971e-90fcb5f57224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539955c4-3ac6-4a35-a23e-afa7be3e72b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
